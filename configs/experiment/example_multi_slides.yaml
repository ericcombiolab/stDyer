# @package _global_

# to execute this experiment run:
# python run.py experiment=example_multi_slides

defaults:
  - override /mode: exp.yaml
  - override /trainer: stdyer.yaml
  - override /model: dgg.yaml
  - override /datamodule: dgg.yaml
  - override /callbacks: stdyer.yaml
  - override /logger: none.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# can also be accessed by loggers
name: "example_multi_slides"
model:
  lr: 0.001

  # learning rate for Gaussian uniform prior, set to 0 will use uniform prior all the time
  prior_lr: 0.2

  gaussian_size: 64
  attention_size: 64

  exp_encoder_channels: [3000, 128, 256, 64]
  exp_decoder_channels: [64, 256, 128, 3000]

  # the number of spatially variable genes to detect for each domain
  detect_svg: 50

  # a simple autoencoder will be used to refine the predicted domain labels (saved in obs["mlp_fit"])
  # the raw predicted domain labels will NOT be overwritten and are saved in obs["pred_labels"]
  # set to null to disable the refinement (recommend to disable it for large datasets with > 100k units)
  refine_ae_channels: [64, 64, 64]

  # weight for neighbor loss, higher weights will make the predicted domains smoother
  exp_neigh_w: 1.

  # starting from this epoch to use fused graph
  patience_start_epoch_pct: 50
  # starting from this epoch to use Gaussian loss
  gaussian_start_epoch_pct: 50
  # parameter of semi-supervised cross-entropy loss
  # higher weights can make the model stick to the domain labels initialized by mclust
  # lower weights can make the model more flexible to update the domain labels
  # you can set it with either "1.001" or "1.01" for testing
  semi_ce_degree: "1.01"

trainer:
  # number of epochs to train
  max_epochs: 200
  # number of GPUs to use
  devices: 1

datamodule:
  # full path to the dataset: data_dir/dataset_dir/sample_id.h5ad
  data_dir: "/home/comp/20481195/Datasets/Visium"
  dataset_dir: "human_dorsolateral_pre-frontal_cortex"
  sample_id: "c_151673_151674_151675_151676"
  # specify data_type as "custom" and use data_file_name for your own dataset
  # data_file_name: "c_151673_151674_151675_151676.h5ad"
  data_type: "10x" # "custom"

  # number of predicted spatial domains
  num_classes: 7
  # number of highly variable genes
  num_hvg: 3000

  # a: number of fixed spatial units to use
  unit_fix_num: 6
  # b: number of dynamic domain units to use
  unit_dynamic_num: 12
  k: 18 # a+b
  rec_neigh_num: 18 # a+b
  forward_neigh_num: 18 # a+b
  max_dynamic_neigh: 18 # a+b

  # use all highly variable genes and perform z-score normalization for model input and reconstruction
  # if you want to use PCA, set it to "pca_scaled"
  in_type: "scaled" # "pca_scaled"
  out_type: "scaled" # "pca_scaled"
  # number of pricipal components to use for PCA
  # n_pc: 50

  # set smaller batch_size (e.g., 256) for dataset
  #     1. with fewer cells to improve convergence speed;
  #     2. with more cells but low-end GPU to avoid "out of GPU memory error";
  # set larger batch_size (e.g., 1024, 4096) for dataset with more cells on high-end GPU to improve GPU utilization;
  batch_size: 1024

  # for 3d spatial coordinates
  # assume s is the least distance between two spatial units
  # For Visium:
  # min=s
  # max=s*√3
  # 1 <= z <= 1.73
  # For stereo-seq
  # min=s
  # max=s*√2
  # 1 <= z <= 1.41
  z_scale: 1.5

  seed: 42
